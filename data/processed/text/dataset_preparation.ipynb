{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "We have our slang_OpenSub.tsv and slang_OpenSub_negative.tsv in our data/raw folder, now we want to combine them into a single dataset so we can train the model. Our goal is to split the combined dataset into 80/10/10 for training/validation/testing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\n",
      "Checking if src exists: True\n",
      "Checking if utils exists: True\n",
      "Checking if config.py exists: True\n"
     ]
    }
   ],
   "source": [
    "# Set Up\n",
    "\n",
    "\n",
    "import os \n",
    "import sys \n",
    "\n",
    "# Get the current directory (where the notebook is)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate up to the project root (LINGO folder)\n",
    "# Assuming notebook is in data/processed/text\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '../../../'))\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Important: Add project root to Python's path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "\n",
    "# Check if src directory exists\n",
    "src_dir = os.path.join(project_root, 'src')\n",
    "utils_dir = os.path.join(src_dir, 'utils')\n",
    "config_file = os.path.join(utils_dir, 'config.py')\n",
    "print(f\"Checking if src exists: {os.path.exists(src_dir)}\")\n",
    "print(f\"Checking if utils exists: {os.path.exists(utils_dir)}\")\n",
    "print(f\"Checking if config.py exists: {os.path.exists(config_file)}\")\n",
    "\n",
    "# Import config\n",
    "from src.utils.config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
    "\n",
    "# Load data\n",
    "#slang_df = pd.read_csv(os.path.join(RAW_DATA_DIR, 'slang_OpenSub.tsv'), sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load the Raw Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\raw\\slang_OpenSub.tsv\n",
      "Loading data from: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\raw\\slang_OpenSub_negatives.tsv\n",
      "Slang dataset shape: (7488, 11)\n",
      "Non-slang dataset shape: (17512, 6)\n",
      "Slang dataset columns: ['SENTENCE', 'FULL_CONTEXT', 'SLANG_TERM', 'ANNOTATOR_CONFIDENCE', 'MOVIE_ID', 'SENT_ID', 'REGION', 'YEAR', 'DEFINITION_SENTENCE', 'DEFINITION_SOURCE_URL', 'LITERAL_PARAPHRASE_OF_SLANG']\n",
      "Non-slang dataset columns: ['SENTENCE', 'FULL_CONTEXT', 'MOVIE_ID', 'SENT_ID', 'REGION', 'YEAR']\n"
     ]
    }
   ],
   "source": [
    "# Load the slang and non-slang datasets\n",
    "slang_file = os.path.join(RAW_DATA_DIR, 'slang_OpenSub.tsv')\n",
    "nonslang_file = os.path.join(RAW_DATA_DIR, 'slang_OpenSub_negatives.tsv')\n",
    "\n",
    "print(f\"Loading data from: {slang_file}\")\n",
    "print(f\"Loading data from: {nonslang_file}\")\n",
    "\n",
    "slang_df = pd.read_csv(slang_file, sep='\\t')\n",
    "nonslang_df = pd.read_csv(nonslang_file, sep='\\t')\n",
    "\n",
    "# Print basic dataset information\n",
    "print(f\"Slang dataset shape: {slang_df.shape}\")\n",
    "print(f\"Non-slang dataset shape: {nonslang_df.shape}\")\n",
    "print(f\"Slang dataset columns: {slang_df.columns.tolist()}\")\n",
    "print(f\"Non-slang dataset columns: {nonslang_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Label and Combine the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (25000, 12)\n",
      "row 15000: SENTENCE                               When I was nine, I really wanted a horse.\n",
      "FULL_CONTEXT                   The horse story? <i> When I was nine, I really...\n",
      "SLANG_TERM                                                                   NaN\n",
      "ANNOTATOR_CONFIDENCE                                                         NaN\n",
      "MOVIE_ID                                                                 6692456\n",
      "SENT_ID                                                                     1556\n",
      "REGION                                                                        US\n",
      "YEAR                                                                        2016\n",
      "DEFINITION_SENTENCE                                                          NaN\n",
      "DEFINITION_SOURCE_URL                                                        NaN\n",
      "LITERAL_PARAPHRASE_OF_SLANG                                                  NaN\n",
      "has_slang                                                                      0\n",
      "Name: 15000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Label and Combine the Datasets\n",
    "\n",
    "# Add label columns (1 for slang, 0 for non-slang)\n",
    "slang_df['has_slang'] = 1\n",
    "nonslang_df['has_slang'] = 0\n",
    "\n",
    "# Combine into a single dataset\n",
    "combined_df = pd.concat([slang_df, nonslang_df], ignore_index=True)\n",
    "\n",
    "#  Check the combined dataset shape, it should have 25000 rows and 12 columns (11 columns from slang_df and the \"has_slang\" column we just defined)\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "\n",
    "\n",
    "#  Check the negative dataset entry here should have NaN in Slang term and annotator_confidence columns and etc.\n",
    "# Columns that non_slang_df didn't have should be NaN\n",
    "print(f\"row 15000: {combined_df.iloc[15000]}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Split the Dataset into Training, Validation, and Test Sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "Non-slang examples: 17512 (70.0%)\n",
      "Slang examples: 7488 (30.0%)\n",
      "\n",
      "Adjusted class distribution:\n",
      "Non-slang examples: 4992 (40.0%)\n",
      "Slang examples: 7488 (60.0%)\n"
     ]
    }
   ],
   "source": [
    "## Split the Dataset into Training, Validation, and Test Sets\n",
    "\n",
    "\n",
    "print(\"Original class distribution:\")\n",
    "slang_count = combined_df['has_slang'].sum()  # Count all 1s (slang examples)\n",
    "nonslang_count = len(combined_df) - slang_count  # Count all 0s (non-slang examples)\n",
    "\n",
    "# Print percentages\n",
    "print(f\"Non-slang examples: {nonslang_count} ({nonslang_count/len(combined_df)*100:.1f}%)\")\n",
    "print(f\"Slang examples: {slang_count} ({slang_count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# For high recall, create a 60/40 distribution (slang/non-slang)\n",
    "target_ratio = 0.6  # Want 60% slang, 40% non-slang\n",
    "\n",
    "# Separating the data based on the combined_df\n",
    "# Create two separate dataframes\n",
    "slang_subset = combined_df[combined_df['has_slang'] == 1]    # Only slang examples\n",
    "nonslang_subset = combined_df[combined_df['has_slang'] == 0] # Only non-slang examples\n",
    "\n",
    "\n",
    "# Keep ALL slang examples and adjust non-slang to achieve target ratio\n",
    "\n",
    "# Calculate required non-slang examples\n",
    "target_nonslang = int(len(slang_subset) * (1-target_ratio) / target_ratio)\n",
    "\"\"\" \n",
    "Math here for the int(len....) calculation above\n",
    "Example: Suppose we have slang_dataset = 2000 examples, and target ratio = 0.6\n",
    "# = int(2000 * 0.4 / 0.6)\n",
    "# = int(2000 * 0.667)\n",
    "# â‰ˆ 1333 non-slang examples\n",
    "\n",
    "# Final distribution:\n",
    "# 2000 slang + 1333 non-slang = 3333 total\n",
    "# 2000/3333 = 60% slang (approximately)\n",
    "# 1333/3333 = 40% non-slang (approximately)\n",
    "\"\"\" \n",
    "# Sample non-slang (no replacement needed)\n",
    "nonslang_adjusted = nonslang_subset.sample(n=target_nonslang, random_state=42)\n",
    "\n",
    "balanced_df = pd.concat([slang_subset, nonslang_adjusted], ignore_index=True)\n",
    "# Shuffle the balanced dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify the new distribution\n",
    "new_slang_count = balanced_df['has_slang'].sum()\n",
    "new_nonslang_count = len(balanced_df) - new_slang_count\n",
    "print(\"\\nAdjusted class distribution:\")\n",
    "print(f\"Non-slang examples: {new_nonslang_count} ({new_nonslang_count/len(balanced_df)*100:.1f}%)\")\n",
    "print(f\"Slang examples: {new_slang_count} ({new_slang_count/len(balanced_df)*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data in the adjusted dataset with 40.0% non-slang and 60.0% slang: 12480\n",
      "\n",
      "Training examples: 9984\n",
      "Validation examples: 1248\n",
      "Test examples: 1248\n",
      "\n",
      "Now we verify the distribution of slang and non-slang data in the training set\n",
      "Class distribution in training set:\n",
      "Non-slang: 3994 (40.0%)\n",
      "Slang: 5990 (60.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of data in the adjusted dataset with {new_nonslang_count/len(balanced_df)*100:.1f}% non-slang and {new_slang_count/len(balanced_df)*100:.1f}% slang: {len(balanced_df)}\")\n",
    "# Split data using 80/10/10 ratio with stratification to maintain class balance\n",
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.2, random_state=42, stratify=balanced_df['has_slang'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['has_slang'])\n",
    "\n",
    "print(f\"\\nTraining examples: {len(train_df)}\")\n",
    "print(f\"Validation examples: {len(val_df)}\")\n",
    "print(f\"Test examples: {len(test_df)}\")\n",
    "\n",
    "# Verify class distribution in splits\n",
    "print(f\"\\nNow we verify the distribution of slang and non-slang data in the training set:\")\n",
    "print(f\"Class distribution in training set:\")\n",
    "print(f\"Non-slang: {(train_df['has_slang'] == 0).sum()} ({(train_df['has_slang'] == 0).sum()/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Slang: {(train_df['has_slang'] == 1).sum()} ({(train_df['has_slang'] == 1).sum()/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Format the Dataset for Instruction Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Choice:\n",
    "\n",
    "**Option 1: Single Model to detect both slang and provide definitions.**\n",
    "Train Qwen model to both detect slang AND provide definitions simultaneously.\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Simpler architecture (one API call)\n",
    "- Lower latency\n",
    "- Works offline\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Limited to definitions available in your training data\n",
    "- Can't easily update definitions without retraining\n",
    "\n",
    "\n",
    "**Option 2: Two-Model Approach**\n",
    "Only fine-tune model to detect slang, then use GPT-4o or other API for definitions and provide more capabilities such as examples, synonyms, and more.\n",
    "\n",
    "Pros:   \n",
    "\n",
    "- Leverages latest models like GPT-4o's comprehensive knowledge base\n",
    "- Can generate creative examples and more detailed explanations\n",
    "- Definitions stay up-to-date with API updates\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Additional API cost (GPT-4o usage)\n",
    "- Higher latency (two sequential API calls)\n",
    "- Requires internet connection\n",
    "- More complex architecture\n",
    "\n",
    "**We will use Option 2 for now.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting examples for instruction tuning...\n",
      "Formatting complete!\n",
      "\n",
      "Example of formatted training data (slang example):\n",
      "Instruction: Identify any slang in this video subtitle: \"You gonna give me a ticket?\"\n",
      "Output: slang detected: gonna\n",
      "slang context: You gonna give me a ticket?\n",
      "\n",
      "Example of formatted training data (non-slang example):\n",
      "Instruction: Identify any slang in this video subtitle: \"No, the woman I lived with.\"\n",
      "Output: no slang detected\n"
     ]
    }
   ],
   "source": [
    "def format_example(row):\n",
    "    \"\"\"\n",
    "    Format a subtitle row into an instruction-output pair for slang detection.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series containing 'SENTENCE', 'has_slang', and optionally 'SLANG_TERM'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Formatted example with instruction, input, and output fields\n",
    "    \n",
    "    Example:\n",
    "        Input row:\n",
    "        {\n",
    "            'SENTENCE': \"That party was lit\",\n",
    "            'has_slang': 1,\n",
    "            'SLANG_TERM': \"lit\"\n",
    "        }\n",
    "        \n",
    "        Returns:\n",
    "        {\n",
    "            \"instruction\": \"Identify any slang in this video subtitle: \\\"That party was lit\\\"\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"slang detected: lit\n",
    "                      slang context: That party was lit\"\n",
    "        }\n",
    "        \n",
    "        For non-slang case (has_slang = 0):\n",
    "        {\n",
    "            \"instruction\": \"Identify any slang in this video subtitle: \\\"The weather is nice\\\"\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"no slang detected\"\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # Clear instruction for slang detection\n",
    "    instruction = f\"Identify any slang in this video subtitle: \\\"{row['SENTENCE']}\\\"\"\n",
    "    \n",
    "    if row['has_slang'] == 1:\n",
    "        # Simple, consistent format that's easy to parse\n",
    "        response = f\"slang detected: {row.get('SLANG_TERM', 'unknown')}\\n\"\n",
    "        response += f\"slang context: {row['SENTENCE']}\"\n",
    "    else:\n",
    "        response = \"no slang detected\"\n",
    "    \n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": \"\",\n",
    "        \"output\": response\n",
    "    }\n",
    "\n",
    "# Apply formatting to each dataset\n",
    "print(\"Formatting examples for instruction tuning...\")\n",
    "\n",
    "# train_formatted = []  # Create empty list\n",
    "# for index, row in train_df.iterrows():\n",
    "#     # index: row number (we don't use it, that's why it's _)\n",
    "#     # row: the actual data for this row\n",
    "#     formatted_data = format_example(row)\n",
    "#     train_formatted.append(formatted_data)\n",
    "\n",
    "train_formatted = [format_example(row) for _, row in train_df.iterrows()]\n",
    "val_formatted = [format_example(row) for _, row in val_df.iterrows()]\n",
    "test_formatted = [format_example(row) for _, row in test_df.iterrows()]\n",
    "print(\"Formatting complete!\")\n",
    "\n",
    "# Show a couple of examples to verify formatting\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nExample of formatted training data (slang example):\")\n",
    "# # Finding slang example\n",
    "# for ex in train_formatted:\n",
    "#     if \"slang detected:\" in ex[\"output\"]:\n",
    "#         slang_example = ex\n",
    "#         break\n",
    "slang_example = next(ex for ex in train_formatted if \"slang detected:\" in ex[\"output\"]) # next() gets the first element from the generator, like give me the first example where\n",
    "print(f\"Instruction: {slang_example['instruction']}\")\n",
    "print(f\"Output: {slang_example['output']}\")\n",
    "\n",
    "print(\"\\nExample of formatted training data (non-slang example):\")\n",
    "nonslang_example = next(ex for ex in train_formatted if \"no slang detected\" in ex[\"output\"])\n",
    "print(f\"Instruction: {nonslang_example['instruction']}\")\n",
    "print(f\"Output: {nonslang_example['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed datasets...\n",
      "Saved training data to: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\processed\\text\\train.json\n",
      "Saved validation data to: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\processed\\text\\val.json\n",
      "Saved test data to: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\processed\\text\\test.json\n",
      "Saved sample data to: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\processed\\text\\sample.json\n"
     ]
    }
   ],
   "source": [
    "# Save Processed Data\n",
    "\n",
    "\n",
    "# Define output paths\n",
    "train_output = os.path.join(PROCESSED_DATA_DIR, 'train.json')\n",
    "val_output = os.path.join(PROCESSED_DATA_DIR, 'val.json')\n",
    "test_output = os.path.join(PROCESSED_DATA_DIR, 'test.json')\n",
    "sample_output = os.path.join(PROCESSED_DATA_DIR, 'sample.json')\n",
    "\n",
    "# Save to JSON files\n",
    "import json\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "with open(train_output, 'w') as f:\n",
    "    json.dump(train_formatted, f)\n",
    "\n",
    "with open(val_output, 'w') as f:\n",
    "    json.dump(val_formatted, f)\n",
    "\n",
    "with open(test_output, 'w') as f:\n",
    "    json.dump(test_formatted, f)\n",
    "\n",
    "# Save a small sample with indentation for easy inspection\n",
    "with open(sample_output, 'w') as f:\n",
    "    # Save 5 examples with formatting for readability\n",
    "    json.dump(train_formatted[:5], f, indent=2)\n",
    "\n",
    "print(f\"Saved training data to: {train_output}\")\n",
    "print(f\"Saved validation data to: {val_output}\")\n",
    "print(f\"Saved test data to: {test_output}\")\n",
    "print(f\"Saved sample data to: {sample_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated training configuration in: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\configs\\training_config.yaml\n",
      "Updated model configuration in: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\configs\\model_config.yaml\n",
      "Updated data paths in: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\configs\\data_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Create Training Configuration and Update existing YAML configuration files in .configs folder\n",
    "import yaml\n",
    "import os\n",
    "from src.utils.config import PROJECT_ROOT\n",
    "\n",
    "def safe_update_yaml(file_path, new_config, section_name):\n",
    "    \"\"\"\n",
    "    Safely update a YAML file's specific section while preserving other sections.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the YAML file\n",
    "        new_config (dict): Dictionary containing the new configuration\n",
    "        section_name (str): Name of the section to update\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the updated file\n",
    "\n",
    "    Example: \n",
    "    If we have a YAML file like this:\n",
    "        text_slang_detector:\n",
    "            lr: 0.0002\n",
    "            epochs: 3\n",
    "            # Other parameters...\n",
    "\n",
    "        video_slang_detector:  # Future addition\n",
    "            lr: 0.0001\n",
    "            epochs: 5\n",
    "            # Other parameters...\n",
    "\n",
    "    When we want to update text_slang_detector, we don't want to accidentally overwrite video_slang_detector and even \n",
    "    accidentally delete other keys and values(like video_slang_detector and all it's parameters) in the YAML file. That's\n",
    "    why we want to use this function.\n",
    "\n",
    "    \"\"\"\n",
    "    # 1. Try to load existing config\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            existing_config = yaml.safe_load(f) or {}\n",
    "    except FileNotFoundError:\n",
    "        # If file doesn't exist, start with empty config\n",
    "        existing_config = {}\n",
    "    \n",
    "    # 2. Update only the specified section\n",
    "    existing_config[section_name] = new_config[section_name]\n",
    "    \n",
    "    # 3. Write back to file\n",
    "    with open(file_path, 'w') as f:\n",
    "        yaml.dump(existing_config, f, default_flow_style=False)\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "# 1. Update training_config.yaml\n",
    "training_params = {\n",
    "    \"text_slang_detector\": { # Such that we can leave key like video_slang_detector in the YAML file without affecting the training\n",
    "        \"lr\": 2e-4,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"max_length\": 512\n",
    "    }\n",
    "}\n",
    "\n",
    "training_config_path = os.path.join(PROJECT_ROOT, 'configs', 'training_config.yaml')\n",
    "safe_update_yaml(training_config_path, training_params, \"text_slang_detector\")\n",
    "print(f\"Updated training configuration in: {training_config_path}\")\n",
    "\n",
    "# 2. Update model_config.yaml\n",
    "model_params = {\n",
    "    \"text_slang_detector\": {  \n",
    "        \"name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        \"lora_params\": {\n",
    "            \"r\": 16,\n",
    "            \"alpha\": 32,\n",
    "            \"dropout\": 0.1,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "        },\n",
    "        \"output_dir\": os.path.join(PROJECT_ROOT, \"models\", \"text_slang_detector\")\n",
    "    }\n",
    "}\n",
    "\n",
    "model_config_path = os.path.join(PROJECT_ROOT, 'configs', 'model_config.yaml')\n",
    "safe_update_yaml(model_config_path, model_params, \"text_slang_detector\")\n",
    "print(f\"Updated model configuration in: {model_config_path}\")\n",
    "\n",
    "# 3. Update data_config.yaml\n",
    "data_params = {\n",
    "    \"text_slang_detector\": {  # Changed from \"slang_detector\" to \"text_slang_detector\"\n",
    "        \"train\": os.path.abspath(train_output),\n",
    "        \"validation\": os.path.abspath(val_output),\n",
    "        \"test\": os.path.abspath(test_output)\n",
    "    }\n",
    "}\n",
    "\n",
    "data_config_path = os.path.join(PROJECT_ROOT, 'configs', 'data_config.yaml')\n",
    "safe_update_yaml(data_config_path, data_params, \"text_slang_detector\")\n",
    "print(f\"Updated data paths in: {data_config_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
