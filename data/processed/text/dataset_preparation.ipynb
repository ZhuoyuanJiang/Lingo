{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "We have our slang_OpenSub.tsv and slang_OpenSub_negative.tsv in our data/raw folder, now we want to combine them into a single dataset so we can train the model. Our goal is to split the combined dataset into 80/10/10 for training/validation/testing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\n",
      "Checking if src exists: True\n",
      "Checking if utils exists: True\n",
      "Checking if config.py exists: True\n"
     ]
    }
   ],
   "source": [
    "# Set Up\n",
    "\n",
    "\n",
    "import os \n",
    "import sys \n",
    "\n",
    "# Get the current directory (where the notebook is)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate up to the project root (LINGO folder)\n",
    "# Assuming notebook is in data/processed/text\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '../../../'))\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Important: Add project root to Python's path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "\n",
    "# Check if src directory exists\n",
    "src_dir = os.path.join(project_root, 'src')\n",
    "utils_dir = os.path.join(src_dir, 'utils')\n",
    "config_file = os.path.join(utils_dir, 'config.py')\n",
    "print(f\"Checking if src exists: {os.path.exists(src_dir)}\")\n",
    "print(f\"Checking if utils exists: {os.path.exists(utils_dir)}\")\n",
    "print(f\"Checking if config.py exists: {os.path.exists(config_file)}\")\n",
    "\n",
    "# Import config\n",
    "from src.utils.config import RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
    "\n",
    "# Load data\n",
    "#slang_df = pd.read_csv(os.path.join(RAW_DATA_DIR, 'slang_OpenSub.tsv'), sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load the Raw Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\raw\\slang_OpenSub.tsv\n",
      "Loading data from: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\raw\\slang_OpenSub_negatives.tsv\n",
      "Slang dataset shape: (7488, 11)\n",
      "Non-slang dataset shape: (17512, 6)\n",
      "Slang dataset columns: ['SENTENCE', 'FULL_CONTEXT', 'SLANG_TERM', 'ANNOTATOR_CONFIDENCE', 'MOVIE_ID', 'SENT_ID', 'REGION', 'YEAR', 'DEFINITION_SENTENCE', 'DEFINITION_SOURCE_URL', 'LITERAL_PARAPHRASE_OF_SLANG']\n",
      "Non-slang dataset columns: ['SENTENCE', 'FULL_CONTEXT', 'MOVIE_ID', 'SENT_ID', 'REGION', 'YEAR']\n"
     ]
    }
   ],
   "source": [
    "# Load the slang and non-slang datasets\n",
    "slang_file = os.path.join(RAW_DATA_DIR, 'slang_OpenSub.tsv')\n",
    "nonslang_file = os.path.join(RAW_DATA_DIR, 'slang_OpenSub_negatives.tsv')\n",
    "\n",
    "print(f\"Loading data from: {slang_file}\")\n",
    "print(f\"Loading data from: {nonslang_file}\")\n",
    "\n",
    "slang_df = pd.read_csv(slang_file, sep='\\t')\n",
    "nonslang_df = pd.read_csv(nonslang_file, sep='\\t')\n",
    "\n",
    "# Print basic dataset information\n",
    "print(f\"Slang dataset shape: {slang_df.shape}\")\n",
    "print(f\"Non-slang dataset shape: {nonslang_df.shape}\")\n",
    "print(f\"Slang dataset columns: {slang_df.columns.tolist()}\")\n",
    "print(f\"Non-slang dataset columns: {nonslang_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Label and Combine the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape: (25000, 12)\n",
      "row 15000: SENTENCE                               When I was nine, I really wanted a horse.\n",
      "FULL_CONTEXT                   The horse story? <i> When I was nine, I really...\n",
      "SLANG_TERM                                                                   NaN\n",
      "ANNOTATOR_CONFIDENCE                                                         NaN\n",
      "MOVIE_ID                                                                 6692456\n",
      "SENT_ID                                                                     1556\n",
      "REGION                                                                        US\n",
      "YEAR                                                                        2016\n",
      "DEFINITION_SENTENCE                                                          NaN\n",
      "DEFINITION_SOURCE_URL                                                        NaN\n",
      "LITERAL_PARAPHRASE_OF_SLANG                                                  NaN\n",
      "has_slang                                                                      0\n",
      "Name: 15000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## Label and Combine the Datasets\n",
    "\n",
    "# Add label columns (1 for slang, 0 for non-slang)\n",
    "slang_df['has_slang'] = 1\n",
    "nonslang_df['has_slang'] = 0\n",
    "\n",
    "# Combine into a single dataset\n",
    "combined_df = pd.concat([slang_df, nonslang_df], ignore_index=True)\n",
    "\n",
    "#  Check the combined dataset shape, it should have 25000 rows and 12 columns (11 columns from slang_df and the \"has_slang\" column we just defined)\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "\n",
    "\n",
    "#  Check the negative dataset entry here should have NaN in Slang term and annotator_confidence columns and etc.\n",
    "# Columns that non_slang_df didn't have should be NaN\n",
    "print(f\"row 15000: {combined_df.iloc[15000]}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Split the Dataset into Training, Validation, and Test Sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "Non-slang examples: 17512 (70.0%)\n",
      "Slang examples: 7488 (30.0%)\n",
      "\n",
      "Adjusted class distribution:\n",
      "Non-slang examples: 4992 (40.0%)\n",
      "Slang examples: 7488 (60.0%)\n"
     ]
    }
   ],
   "source": [
    "## Split the Dataset into Training, Validation, and Test Sets\n",
    "\n",
    "\n",
    "print(\"Original class distribution:\")\n",
    "slang_count = combined_df['has_slang'].sum()  # Count all 1s (slang examples)\n",
    "nonslang_count = len(combined_df) - slang_count  # Count all 0s (non-slang examples)\n",
    "\n",
    "# Print percentages\n",
    "print(f\"Non-slang examples: {nonslang_count} ({nonslang_count/len(combined_df)*100:.1f}%)\")\n",
    "print(f\"Slang examples: {slang_count} ({slang_count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# For high recall, create a 60/40 distribution (slang/non-slang)\n",
    "target_ratio = 0.6  # Want 60% slang, 40% non-slang\n",
    "\n",
    "# Separating the data based on the combined_df\n",
    "# Create two separate dataframes\n",
    "slang_subset = combined_df[combined_df['has_slang'] == 1]    # Only slang examples\n",
    "nonslang_subset = combined_df[combined_df['has_slang'] == 0] # Only non-slang examples\n",
    "\n",
    "\n",
    "# Keep ALL slang examples and adjust non-slang to achieve target ratio\n",
    "\n",
    "# Calculate required non-slang examples\n",
    "target_nonslang = int(len(slang_subset) * (1-target_ratio) / target_ratio)\n",
    "\"\"\" \n",
    "Math here for the int(len....) calculation above\n",
    "Example: Suppose we have slang_dataset = 2000 examples, and target ratio = 0.6\n",
    "# = int(2000 * 0.4 / 0.6)\n",
    "# = int(2000 * 0.667)\n",
    "# â‰ˆ 1333 non-slang examples\n",
    "\n",
    "# Final distribution:\n",
    "# 2000 slang + 1333 non-slang = 3333 total\n",
    "# 2000/3333 = 60% slang (approximately)\n",
    "# 1333/3333 = 40% non-slang (approximately)\n",
    "\"\"\" \n",
    "# Sample non-slang (no replacement needed)\n",
    "nonslang_adjusted = nonslang_subset.sample(n=target_nonslang, random_state=42)\n",
    "\n",
    "balanced_df = pd.concat([slang_subset, nonslang_adjusted], ignore_index=True)\n",
    "# Shuffle the balanced dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify the new distribution\n",
    "new_slang_count = balanced_df['has_slang'].sum()\n",
    "new_nonslang_count = len(balanced_df) - new_slang_count\n",
    "print(\"\\nAdjusted class distribution:\")\n",
    "print(f\"Non-slang examples: {new_nonslang_count} ({new_nonslang_count/len(balanced_df)*100:.1f}%)\")\n",
    "print(f\"Slang examples: {new_slang_count} ({new_slang_count/len(balanced_df)*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data in the adjusted dataset with 40.0% non-slang and 60.0% slang: 12480\n",
      "\n",
      "Training examples: 9984\n",
      "Validation examples: 1248\n",
      "Test examples: 1248\n",
      "\n",
      "Now we verify the distribution of slang and non-slang data in the training set\n",
      "Class distribution in training set:\n",
      "Non-slang: 3994 (40.0%)\n",
      "Slang: 5990 (60.0%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of data in the adjusted dataset with {new_nonslang_count/len(balanced_df)*100:.1f}% non-slang and {new_slang_count/len(balanced_df)*100:.1f}% slang: {len(balanced_df)}\")\n",
    "# Split data using 80/10/10 ratio with stratification to maintain class balance\n",
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.2, random_state=42, stratify=balanced_df['has_slang'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['has_slang'])\n",
    "\n",
    "print(f\"\\nTraining examples: {len(train_df)}\")\n",
    "print(f\"Validation examples: {len(val_df)}\")\n",
    "print(f\"Test examples: {len(test_df)}\")\n",
    "\n",
    "# Verify class distribution in splits\n",
    "print(f\"\\nNow we verify the distribution of slang and non-slang data in the training set:\")\n",
    "print(f\"Class distribution in training set:\")\n",
    "print(f\"Non-slang: {(train_df['has_slang'] == 0).sum()} ({(train_df['has_slang'] == 0).sum()/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Slang: {(train_df['has_slang'] == 1).sum()} ({(train_df['has_slang'] == 1).sum()/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Format the Dataset for Instruction Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
