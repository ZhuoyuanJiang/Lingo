{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wordfreq in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (3.1.1)\n",
      "Requirement already satisfied: ftfy>=6.1 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from wordfreq) (6.3.1)\n",
      "Requirement already satisfied: langcodes>=3.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from wordfreq) (3.5.0)\n",
      "Requirement already satisfied: locate<2.0.0,>=1.1.1 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from wordfreq) (1.1.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from wordfreq) (1.1.0)\n",
      "Requirement already satisfied: regex>=2023.10.3 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from wordfreq) (2024.9.11)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes>=3.0->wordfreq) (75.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.5-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.17)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.3)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy) (2.1.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.5-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 9.7/11.8 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.8 MB 60.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 25.5 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.6/632.6 kB 24.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 31.6 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.3.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.3/6.3 MB 54.5 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, mdurl, cloudpathlib, catalogue, blis, srsly, smart-open, preshed, markdown-it-py, rich, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 rich-14.0.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install wordfreq\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\jiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\jiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown, gutenberg\n",
    "nltk.download('brown')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset:\n",
      "                                                 text  has_slang\n",
      "0   That party last night was lit, everyone had a ...          1\n",
      "1             He ghosted me after our date last week.          1\n",
      "2   This project is going to be dope when it's fin...          1\n",
      "3             I'm totally vibing with this new music.          1\n",
      "4   The teacher was throwing shade at the students...          1\n",
      "5                    That's sus, I don't believe you.          1\n",
      "6    The new iPhone is fire, I can't wait to get one.          1\n",
      "7          Stop flexing your new car on social media.          1\n",
      "8                The weather is quite pleasant today.          0\n",
      "9       I need to finish my homework before tomorrow.          0\n",
      "10  The movie received positive reviews from critics.          0\n",
      "11           She plays tennis every Saturday morning.          0\n",
      "12  They decided to renovate their kitchen last su...          0\n"
     ]
    }
   ],
   "source": [
    "# Create test sentences with known slang\n",
    "test_sentences_with_slang = [\n",
    "    \"That party last night was lit, everyone had a great time.\",\n",
    "    \"He ghosted me after our date last week.\",\n",
    "    \"This project is going to be dope when it's finished.\",\n",
    "    \"I'm totally vibing with this new music.\",\n",
    "    \"The teacher was throwing shade at the students who didn't do their homework.\",\n",
    "    \"That's sus, I don't believe you.\",\n",
    "    \"The new iPhone is fire, I can't wait to get one.\",\n",
    "    \"Stop flexing your new car on social media.\"\n",
    "]\n",
    "\n",
    "# Create control sentences without slang\n",
    "test_sentences_without_slang = [\n",
    "    \"The weather is quite pleasant today.\",\n",
    "    \"I need to finish my homework before tomorrow.\",\n",
    "    \"The movie received positive reviews from critics.\",\n",
    "    \"She plays tennis every Saturday morning.\",\n",
    "    \"They decided to renovate their kitchen last summer.\"\n",
    "]\n",
    "\n",
    "# Combine into a test dataset\n",
    "test_data = pd.DataFrame({\n",
    "    'text': test_sentences_with_slang + test_sentences_without_slang,\n",
    "    'has_slang': [1]*len(test_sentences_with_slang) + [0]*len(test_sentences_without_slang)\n",
    "})\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     --------------------------------------  12.6/12.8 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 42.2 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "--- Approach 1: spaCy with WordNet ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: That party last night was lit, everyone had a great time.\n",
      "Has slang (ground truth): 1\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: He ghosted me after our date last week.\n",
      "Has slang (ground truth): 1\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: This project is going to be dope when it's finished.\n",
      "Has slang (ground truth): 1\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: I'm totally vibing with this new music.\n",
      "Has slang (ground truth): 1\n",
      "Detected potential slang: ['vibing']\n",
      "---\n",
      "Sentence: The teacher was throwing shade at the students who didn't do their homework.\n",
      "Has slang (ground truth): 1\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: That's sus, I don't believe you.\n",
      "Has slang (ground truth): 1\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: The new iPhone is fire, I can't wait to get one.\n",
      "Has slang (ground truth): 1\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: Stop flexing your new car on social media.\n",
      "Has slang (ground truth): 1\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: The weather is quite pleasant today.\n",
      "Has slang (ground truth): 0\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: I need to finish my homework before tomorrow.\n",
      "Has slang (ground truth): 0\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: The movie received positive reviews from critics.\n",
      "Has slang (ground truth): 0\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: She plays tennis every Saturday morning.\n",
      "Has slang (ground truth): 0\n",
      "Detected potential slang: []\n",
      "---\n",
      "Sentence: They decided to renovate their kitchen last summer.\n",
      "Has slang (ground truth): 0\n",
      "Detected potential slang: []\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Approach 1: Using spaCy with WordNet for slang detection\n",
    "\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "print(\"\\n--- Approach 1: spaCy with WordNet ---\")\n",
    "\n",
    "try:\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def detect_slang_spacy_wordnet(text):\n",
    "        \"\"\"\n",
    "        Detect potential slang words using spaCy and WordNet\n",
    "        \n",
    "        Strategy:\n",
    "        1. Use spaCy for tokenization and POS tagging\n",
    "        2. Check if the word exists in WordNet\n",
    "        3. Words not in WordNet and not proper nouns are potential slang\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "        results = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip punctuation, numbers, and short words\n",
    "            if not token.is_alpha or len(token.text) <= 2:\n",
    "                continue\n",
    "                \n",
    "            # Skip proper nouns and stop words\n",
    "            if token.pos_ == \"PROPN\" or token.is_stop:\n",
    "                continue\n",
    "                \n",
    "            word = token.text.lower()\n",
    "            \n",
    "            # Check if word exists in WordNet\n",
    "            synsets = wordnet.synsets(word)\n",
    "            \n",
    "            # If word doesn't exist in WordNet, it might be slang\n",
    "            if not synsets:\n",
    "                results.append({\n",
    "                    \"word\": token.text,\n",
    "                    \"pos\": token.pos_,\n",
    "                    \"confidence\": 0.7\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Test on our dataset\n",
    "    for i, row in test_data.iterrows():\n",
    "        slang_words = detect_slang_spacy_wordnet(row['text'])\n",
    "        print(f\"Sentence: {row['text']}\")\n",
    "        print(f\"Has slang (ground truth): {row['has_slang']}\")\n",
    "        print(f\"Detected potential slang: {[item['word'] for item in slang_words]}\")\n",
    "        print(\"---\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading spaCy: {e}\")\n",
    "    print(\"Skipping spaCy approach\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Approach 2: Zero-shot Classification with BART ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: That party last night was lit, everyone had a great time.\n",
      "Detected slang: []\n",
      "---\n",
      "Sentence: He ghosted me after our date last week.\n",
      "Detected slang: ['ghosted']\n",
      "---\n",
      "Sentence: This project is going to be dope when it's finished.\n",
      "Detected slang: ['dope']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Approach 2: Zero-shot classification with BART\n",
    "\n",
    "nltk.download('punkt_tab') # download for word_tokenize\n",
    "\n",
    "print(\"\\n--- Approach 2: Zero-shot Classification with BART ---\")\n",
    "\n",
    "try:\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "    \n",
    "    # Function to identify potential slang words\n",
    "    def identify_slang_words_zero_shot(text, classifier):\n",
    "        # First classify the whole sentence\n",
    "        sentence_result = classifier(text, [\"contains slang\", \"formal language\"])\n",
    "        sentence_has_slang = sentence_result['labels'][0] == \"contains slang\"\n",
    "        \n",
    "        if not sentence_has_slang:\n",
    "            return []\n",
    "        \n",
    "        # If sentence likely has slang, check individual words\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word.isalpha() and len(word) > 2]\n",
    "        \n",
    "        results = []\n",
    "        for word in words:\n",
    "            # Create a simple context to check if the word is slang\n",
    "            context = f\"The word '{word}' is\"\n",
    "            word_result = classifier(context, [\"slang\", \"standard English\"])\n",
    "            \n",
    "            if word_result['labels'][0] == \"slang\" and word_result['scores'][0] > 0.9:\n",
    "                results.append({\n",
    "                    \"word\": word,\n",
    "                    \"confidence\": word_result['scores'][0]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Test on a few examples (this can be slow)\n",
    "    for sentence in test_data['text'][:3]:  # Just test first 3 to save time\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        slang_words = identify_slang_words_zero_shot(sentence, classifier)\n",
    "        print(f\"Detected slang: {[item['word'] for item in slang_words]}\")\n",
    "        print(\"---\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error with zero-shot classification: {e}\")\n",
    "    print(\"Skipping zero-shot approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Approach 3: Text Classification with HuggingFace ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: That party last night was lit, everyone had a great time.\n",
      "Detected slang: []\n",
      "---\n",
      "Sentence: He ghosted me after our date last week.\n",
      "Detected slang: []\n",
      "---\n",
      "Sentence: This project is going to be dope when it's finished.\n",
      "Detected slang: []\n",
      "---\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Approach 3: Using HuggingFace's text-classification models\n",
    "print(\"\\n--- Approach 3: Text Classification with HuggingFace ---\")\n",
    "\n",
    "try:\n",
    "    # Load a sentiment analysis model (as a proxy for formality detection)\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "    \n",
    "    def detect_slang_with_sentiment(text):\n",
    "        \"\"\"\n",
    "        Use sentiment analysis as a proxy for detecting informal language\n",
    "        \"\"\"\n",
    "        # Analyze whole sentence\n",
    "        result = sentiment_analyzer(text)\n",
    "        \n",
    "        # Tokenize and check individual words\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word.isalpha() and len(word) > 2]\n",
    "        \n",
    "        # Check each word in context\n",
    "        results = []\n",
    "        for word in words:\n",
    "            # Skip common words\n",
    "            if word.lower() in ['the', 'and', 'that', 'this', 'with', 'from', 'have', 'they']:\n",
    "                continue\n",
    "                \n",
    "            # Create contexts to test the word\n",
    "            formal_context = f\"The {word} is appropriate for formal writing.\"\n",
    "            informal_context = f\"The {word} is slang.\"\n",
    "            \n",
    "            formal_result = sentiment_analyzer(formal_context)[0]\n",
    "            informal_result = sentiment_analyzer(informal_context)[0]\n",
    "            \n",
    "            # If the word is more likely to be described as slang\n",
    "            if (informal_result['label'] == 'POSITIVE' and \n",
    "                informal_result['score'] > 0.7 and\n",
    "                formal_result['label'] == 'NEGATIVE'):\n",
    "                \n",
    "                results.append({\n",
    "                    \"word\": word,\n",
    "                    \"confidence\": informal_result['score']\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Test on a few examples\n",
    "    for sentence in test_data['text'][:3]:  # Just test first 3 to save time\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        slang_words = detect_slang_with_sentiment(sentence)\n",
    "        print(f\"Detected slang: {[item['word'] for item in slang_words]}\")\n",
    "        print(\"---\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error with sentiment analysis: {e}\")\n",
    "    print(\"Skipping sentiment analysis approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 4: Using GPT API for slang detection\n",
    "print(\"\\n--- Approach 4: Using GPT API ---\")\n",
    "print(\"Note: This requires an OpenAI API key. Showing example code only.\")\n",
    "\n",
    "def detect_slang_with_gpt(text, api_key=None):\n",
    "    \"\"\"\n",
    "    Use GPT API to detect slang words in text\n",
    "    \n",
    "    Note: This is example code and requires an API key to run\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        return \"API key required\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following text and identify any slang words or phrases:\n",
    "    \n",
    "    \"{text}\"\n",
    "    \n",
    "    For each slang word or phrase, provide:\n",
    "    1. The word or phrase\n",
    "    2. A brief explanation of what it means\n",
    "    3. A confidence score (0-1) of how certain you are it's slang\n",
    "    \n",
    "    Format your response as a JSON object with a list of identified slang terms.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.3,\n",
    "        \"response_format\": {\"type\": \"json_object\"}\n",
    "    }\n",
    "    \n",
    "    # This would make the actual API call\n",
    "    # response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=data)\n",
    "    # return response.json()\n",
    "    \n",
    "    # Instead, return example output\n",
    "    example_output = {\n",
    "        \"slang_terms\": [\n",
    "            {\n",
    "                \"term\": \"lit\",\n",
    "                \"meaning\": \"Exciting, excellent, or amazing\",\n",
    "                \"confidence\": 0.95\n",
    "            },\n",
    "            {\n",
    "                \"term\": \"ghosted\",\n",
    "                \"meaning\": \"When someone cuts off all communication without explanation\",\n",
    "                \"confidence\": 0.98\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return example_output\n",
    "\n",
    "# Show example output for GPT API\n",
    "print(\"Example GPT API output for 'That party last night was lit, everyone ghosted early though.':\")\n",
    "example_result = detect_slang_with_gpt(\"That party last night was lit, everyone ghosted early though.\")\n",
    "print(json.dumps(example_result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 5: Using a dictionary-based approach with common slang terms\n",
    "print(\"\\n--- Approach 5: Dictionary of Common Slang Terms ---\")\n",
    "\n",
    "# Create a small dictionary of common slang terms\n",
    "common_slang = {\n",
    "    \"lit\": \"Exciting or excellent\",\n",
    "    \"dope\": \"Excellent, very good\",\n",
    "    \"fire\": \"Excellent or exciting\",\n",
    "    \"ghosted\": \"When someone cuts off all communication\",\n",
    "    \"vibing\": \"Enjoying or connecting with something\",\n",
    "    \"shade\": \"Subtle disrespect or criticism\",\n",
    "    \"sus\": \"Suspicious or questionable\",\n",
    "    \"flex\": \"To show off\",\n",
    "    \"salty\": \"Bitter or angry\",\n",
    "    \"cap\": \"A lie or to lie\",\n",
    "    \"bet\": \"Agreement or affirmation\",\n",
    "    \"fam\": \"Friends or family\",\n",
    "    \"goat\": \"Greatest of all time\",\n",
    "    \"lowkey\": \"Secretly or subtly\",\n",
    "    \"highkey\": \"Obviously or clearly\",\n",
    "    \"slay\": \"To do something exceptionally well\",\n",
    "    \"tea\": \"Gossip or information\",\n",
    "    \"yeet\": \"To throw something forcefully\",\n",
    "    \"woke\": \"Aware of social issues\",\n",
    "    \"basic\": \"Mainstream or unoriginal\"\n",
    "}\n",
    "\n",
    "def detect_slang_dictionary(text, slang_dict):\n",
    "    \"\"\"\n",
    "    Detect slang words using a dictionary of common slang terms\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Find matches in our slang dictionary\n",
    "    results = []\n",
    "    for word in words:\n",
    "        if word in slang_dict:\n",
    "            results.append({\n",
    "                \"word\": word,\n",
    "                \"meaning\": slang_dict[word],\n",
    "                \"confidence\": 1.0  # High confidence for dictionary matches\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test on our dataset\n",
    "for i, row in test_data.iterrows():\n",
    "    slang_words = detect_slang_dictionary(row['text'], common_slang)\n",
    "    print(f\"Sentence: {row['text']}\")\n",
    "    print(f\"Has slang (ground truth): {row['has_slang']}\")\n",
    "    print(f\"Detected slang: {[item['word'] for item in slang_words]}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Evaluate the dictionary approach\n",
    "test_data['detected_slang'] = test_data['text'].apply(lambda x: detect_slang_dictionary(x, common_slang))\n",
    "test_data['predicted_has_slang'] = test_data['detected_slang'].apply(lambda x: len(x) > 0)\n",
    "accuracy = (test_data['has_slang'] == test_data['predicted_has_slang']).mean()\n",
    "print(f\"Dictionary approach accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of approaches\n",
    "print(\"\\n--- Summary of Slang Detection Approaches ---\")\n",
    "print(\"\"\"\n",
    "1. spaCy with WordNet:\n",
    "   - Pros: No external API needed, linguistically informed\n",
    "   - Cons: May flag legitimate rare words as slang, misses context\n",
    "\n",
    "2. Zero-shot Classification with BART:\n",
    "   - Pros: Can detect slang without explicit training\n",
    "   - Cons: Slow, less accurate for individual words\n",
    "\n",
    "3. Text Classification with HuggingFace:\n",
    "   - Pros: Uses pre-trained models\n",
    "   - Cons: Models not specifically trained for slang detection\n",
    "\n",
    "4. GPT API:\n",
    "   - Pros: High accuracy, provides explanations\n",
    "   - Cons: Requires API key, costs money, slower\n",
    "\n",
    "5. Dictionary-based Approach:\n",
    "   - Pros: Fast, simple, high precision\n",
    "   - Cons: Limited to known slang terms, requires manual updates\n",
    "\n",
    "Recommendation:\n",
    "For a production system, a hybrid approach combining dictionary lookup with\n",
    "either GPT API or a fine-tuned BERT model would likely provide the best results.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
