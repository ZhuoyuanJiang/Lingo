{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca69445",
   "metadata": {},
   "source": [
    "# Slang‑Detector Fine‑Tuning Notebook\n",
    "This notebook is generated from your **train_text_model.py** script so you can run each stage interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832ecc1",
   "metadata": {},
   "source": [
    "# ## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076cd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (4.43.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from peft) (0.26.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1740331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.15.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\jiang\\appdata\\roaming\\python\\python312\\site-packages (from scipy) (2.2.5)\n",
      "Using cached scipy-1.15.2-cp312-cp312-win_amd64.whl (40.9 MB)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.15.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scprep 1.2.3 requires pandas<2.1,>=0.25, but you have pandas 2.2.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install urllib3==1.26.17\n",
    "# !pip install pandas\n",
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fac8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376fd796",
   "metadata": {},
   "source": [
    "# ## 2. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e7ea048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yaml_cfg(path: Path, section: str):\n",
    "    \"\"\"Loads a section from a YAML file with error checking.\"\"\"\n",
    "    if not path.is_file():\n",
    "        logging.error(f\"Configuration file not found: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            data = yaml.safe_load(f) or {}\n",
    "    except yaml.YAMLError as e:\n",
    "        logging.error(f\"Error parsing YAML file {path}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading YAML file {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if section not in data:\n",
    "        logging.error(f\"Section '{section}' missing in configuration file: {path}\")\n",
    "        return None\n",
    "    return data[section]\n",
    "\n",
    "def create_prompt(instruction, output):\n",
    "    # Format specific to Qwen2.5-Instruct model\n",
    "    return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This chunk will lead to bugs later but it uses multiple workers for multiprocessing,\n",
    "# # so we are keeping it here so that we might want to debug later \n",
    "# # So in next chunk, we will use num_proc=1 to get the tokenization done first.\n",
    "\n",
    "# def tokenize_dataset(dataset, tokenizer, max_length):\n",
    "#     \"\"\"Tokenizes the dataset using the specified format with batching and parallelism.\"\"\"\n",
    "#     import logging  # Import inside the function so it's available to workers\n",
    "\n",
    "#     logging.info(f\"Tokenizing dataset with max_length={max_length}...\")\n",
    "\n",
    "#     # Define create_prompt inside tokenize_dataset with the same name as the global function \n",
    "#     # we define this because we want multi-workers to be able to access this function so we don't have error when we\n",
    "#     # call this tokenize_dataset function in multi-workers\n",
    "#     def create_prompt(instruction, output):\n",
    "#         # Format specific to Qwen2.5-Instruct model\n",
    "#         return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "\n",
    "#     def preprocess_function(examples):\n",
    "#         # Add debugging to see what's coming in\n",
    "#         if not isinstance(examples, dict):\n",
    "#             logging.error(f\"Expected examples to be a dict, got {type(examples)}\")\n",
    "#             logging.error(f\"Examples content: {examples}\")\n",
    "#             # Handle the error case\n",
    "#             return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "#         # Check if the expected keys exist\n",
    "#         if 'instruction' not in examples or 'output' not in examples:\n",
    "#             logging.error(f\"Missing expected keys. Available keys: {examples.keys()}\")\n",
    "#             # Handle the error case\n",
    "#             return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "#         # Now process normally\n",
    "#         prompts = [\n",
    "#             create_prompt(instr, out) \n",
    "#             for instr, out in zip(examples['instruction'], examples['output'])\n",
    "#         ]\n",
    "\n",
    "#         tokenized_outputs = tokenizer(\n",
    "#             prompts,\n",
    "#             max_length=max_length,\n",
    "#             truncation=True,\n",
    "#             padding=False,\n",
    "#         )\n",
    "#         tokenized_outputs[\"labels\"] = tokenized_outputs[\"input_ids\"].copy()\n",
    "#         return tokenized_outputs\n",
    "\n",
    "#     try:\n",
    "#         # Use batching and multiprocessing for speed\n",
    "#         num_proc = max(os.cpu_count() // 2, 1)\n",
    "#         logging.info(f\"Using {num_proc} processes for tokenization.\")\n",
    "#         tokenized_ds = dataset.map(\n",
    "#             preprocess_function,\n",
    "#             batched=True,\n",
    "#             num_proc=num_proc,\n",
    "#             remove_columns=dataset.column_names,\n",
    "#             desc=\"Running tokenizer on dataset\",\n",
    "#         )\n",
    "#         logging.info(\"Tokenization complete.\")\n",
    "#         return tokenized_ds\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error during dataset tokenization: {e}\", exc_info=True)\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0a5cc4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This chunk is map without batching or multiprocessing for now\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_length):\n",
    "    \"\"\"Tokenizes the dataset using the specified format.\"\"\"\n",
    "    import logging\n",
    "    logging.info(f\"Tokenizing dataset with max_length={max_length}...\")\n",
    "    \n",
    "    print(f\"Starting tokenization of dataset with {len(dataset)} examples\")\n",
    "    print(f\"Dataset columns: {dataset.column_names}\")\n",
    "    \n",
    "    # Process one example at a time\n",
    "    def process_example(example):\n",
    "        # Create the prompt\n",
    "        prompt = f\"<|im_start|>user\\n{example['instruction']}<|im_end|>\\n<|im_start|>assistant\\n{example['output']}<|im_end|>\"\n",
    "        \n",
    "        # Tokenize\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "        \n",
    "        # Add labels\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "    \n",
    "    try:\n",
    "        # Map without batching or multiprocessing for now\n",
    "        tokenized_ds = dataset.map(\n",
    "            process_example,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        \n",
    "        print(f\"Tokenization complete. Result size: {len(tokenized_ds)}\")\n",
    "        if len(tokenized_ds) > 0:\n",
    "            print(f\"Tokenized dataset features: {tokenized_ds.features}\")\n",
    "            print(f\"First example keys: {list(tokenized_ds[0].keys())}\")\n",
    "        else:\n",
    "            print(\"WARNING: Empty tokenized dataset returned\")\n",
    "        \n",
    "        logging.info(\"Tokenization complete.\")\n",
    "        return tokenized_ds\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during tokenization: {e}\", exc_info=True)\n",
    "        print(f\"Error during tokenization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae2f6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized version for batching and parallelism\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer, max_length):\n",
    "    \"\"\"Tokenizes the dataset using the specified format with batching and parallelism.\"\"\"\n",
    "    import logging\n",
    "    logging.info(f\"Tokenizing dataset with max_length={max_length}...\")\n",
    "    \n",
    "    # Define create_prompt inside the function for multiprocessing\n",
    "    def create_prompt(instruction, output):\n",
    "        return f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{output}<|im_end|>\"\n",
    "    \n",
    "    # Process examples in batches\n",
    "    def process_batch(examples):\n",
    "        # Create prompts for each example in the batch\n",
    "        prompts = [\n",
    "            create_prompt(instr, out) \n",
    "            for instr, out in zip(examples['instruction'], examples['output'])\n",
    "        ]\n",
    "        \n",
    "        # Tokenize all prompts in the batch\n",
    "        tokenized_outputs = tokenizer(\n",
    "            prompts,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "        )\n",
    "        \n",
    "        # Add labels for causal language modeling\n",
    "        tokenized_outputs[\"labels\"] = tokenized_outputs[\"input_ids\"].copy()\n",
    "        return tokenized_outputs\n",
    "    \n",
    "    try:\n",
    "        # Use batching and multiprocessing for speed\n",
    "        num_proc = max(os.cpu_count() // 2, 1)\n",
    "        logging.info(f\"Using {num_proc} processes for tokenization.\")\n",
    "        \n",
    "        tokenized_ds = dataset.map(\n",
    "            process_batch,\n",
    "            batched=True,\n",
    "            batch_size=100,  # Reasonable batch size\n",
    "            num_proc=num_proc,\n",
    "            remove_columns=dataset.column_names,\n",
    "            desc=\"Map: \",\n",
    "        )\n",
    "        \n",
    "        logging.info(\"Tokenization complete.\")\n",
    "        return tokenized_ds\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during tokenization: {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129fb46e",
   "metadata": {},
   "source": [
    "## 3. Load Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0d68d608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\n",
      "Checking if src exists: True\n",
      "Checking if utils exists: True\n",
      "Checking if config.py exists: True\n",
      "Configuration Section: text_slang_detector\n",
      "Using 4-bit quantization: True\n",
      "Using bfloat16 precision: True\n",
      "Max training samples: All\n",
      "Max evaluation samples: All\n"
     ]
    }
   ],
   "source": [
    "# ## 3. Configuration Parameters\n",
    "# Set these parameters to control the training process\n",
    "\n",
    "\n",
    "# Get the current directory (where the notebook is)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate up to the project root (LINGO folder)\n",
    "# Assuming notebook is in data/processed/text\n",
    "project_root = os.path.abspath(os.path.join(current_dir,'../'))\n",
    "print(f\"Project root: {project_root}\")\n",
    "\n",
    "# Important: Add project root to Python's path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Check if src directory exists\n",
    "src_dir = os.path.join(project_root, 'src')\n",
    "utils_dir = os.path.join(src_dir, 'utils')\n",
    "config_file = os.path.join(utils_dir, 'config.py')\n",
    "print(f\"Checking if src exists: {os.path.exists(src_dir)}\")\n",
    "print(f\"Checking if utils exists: {os.path.exists(utils_dir)}\")\n",
    "print(f\"Checking if config.py exists: {os.path.exists(config_file)}\")\n",
    "\n",
    "# Import config\n",
    "from src.utils.config import RAW_DATA_DIR, PROCESSED_DATA_DIR, CONFIGS_DIR\n",
    "\n",
    "\n",
    "# %%\n",
    "# Set parameters\n",
    "section = \"text_slang_detector\"  # Section name in config files\n",
    "configs_dir = Path(CONFIGS_DIR)      # Directory containing config YAML files\n",
    "use_4bit = True                    # Whether to use 4-bit quantization\n",
    "bf16 = True                        # Whether to use bfloat16 precision\n",
    "max_train_samples = None           # Limit training samples (set to number for testing)\n",
    "max_eval_samples = None            # Limit evaluation samples (set to number for testing)\n",
    "\n",
    "# Print current settings\n",
    "print(f\"Configuration Section: {section}\")\n",
    "print(f\"Using 4-bit quantization: {use_4bit}\")\n",
    "print(f\"Using bfloat16 precision: {bf16}\")\n",
    "print(f\"Max training samples: {max_train_samples or 'All'}\")\n",
    "print(f\"Max evaluation samples: {max_eval_samples or 'All'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541eef17",
   "metadata": {},
   "source": [
    "# ## 4. Load Configurations\n",
    "## Load training, model and data configurations from YAML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "77086533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded configurations:\n",
      "\n",
      "Training Configuration:\n",
      "{\n",
      "  \"batch_size\": 4,\n",
      "  \"epochs\": 3,\n",
      "  \"gradient_accumulation_steps\": 4,\n",
      "  \"lr\": 0.0002,\n",
      "  \"max_length\": 512,\n",
      "  \"warmup_ratio\": 0.03\n",
      "}\n",
      "\n",
      "Model Configuration:\n",
      "{\n",
      "  \"lora_params\": {\n",
      "    \"alpha\": 32,\n",
      "    \"dropout\": 0.1,\n",
      "    \"r\": 16,\n",
      "    \"target_modules\": [\n",
      "      \"q_proj\",\n",
      "      \"k_proj\",\n",
      "      \"v_proj\",\n",
      "      \"o_proj\"\n",
      "    ]\n",
      "  },\n",
      "  \"name\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
      "  \"output_dir\": \"c:\\\\Users\\\\jiang\\\\Desktop\\\\Projects\\\\Lingo\\\\models\\\\text_models\"\n",
      "}\n",
      "\n",
      "Data Configuration:\n",
      "{\n",
      "  \"test\": \"c:\\\\Users\\\\jiang\\\\Desktop\\\\Projects\\\\Lingo\\\\data\\\\processed\\\\text\\\\test.json\",\n",
      "  \"train\": \"c:\\\\Users\\\\jiang\\\\Desktop\\\\Projects\\\\Lingo\\\\data\\\\processed\\\\text\\\\train.json\",\n",
      "  \"validation\": \"c:\\\\Users\\\\jiang\\\\Desktop\\\\Projects\\\\Lingo\\\\data\\\\processed\\\\text\\\\val.json\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load configurations\n",
    "try:\n",
    "    # Assume script is run from project root\n",
    "    project_root = configs_dir.parent\n",
    "    train_cfg = yaml_cfg(configs_dir / \"training_config.yaml\", section)\n",
    "    model_cfg = yaml_cfg(configs_dir / \"model_config.yaml\", section)\n",
    "    data_cfg = yaml_cfg(configs_dir / \"data_config.yaml\", section)\n",
    "    \n",
    "    print(\"Successfully loaded configurations:\")\n",
    "    print(\"\\nTraining Configuration:\")\n",
    "    print(json.dumps(train_cfg, indent=2))\n",
    "    print(\"\\nModel Configuration:\")\n",
    "    print(json.dumps(model_cfg, indent=2))\n",
    "    print(\"\\nData Configuration:\")\n",
    "    print(json.dumps(data_cfg, indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configurations: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c6e27",
   "metadata": {},
   "source": [
    "# 5. Process Paths\n",
    "## Resolve and validate data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "575e1837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiang\\Desktop\\Projects\\Lingo\n"
     ]
    }
   ],
   "source": [
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "19b0ffa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using output directory from config: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\models\\text_models\n",
      "Output directory: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\models\\text_models\n",
      "train path: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\processed\\text\\train.json\n",
      "  - Exists: True\n",
      "validation path: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\processed\\text\\val.json\n",
      "  - Exists: True\n",
      "test path: c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\processed\\text\\test.json\n",
      "  - Exists: True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Set up output directory\n",
    "    output_dir_from_config = model_cfg.get(\"output_dir\")\n",
    "    if output_dir_from_config is None:\n",
    "        print(f\"WARNING: No output_dir specified in config, falling back to 'models/{section}_output'\")\n",
    "        output_dir = project_root / f\"models/{section}_output\"\n",
    "    else:\n",
    "        output_dir = project_root / output_dir_from_config\n",
    "        print(f\"Using output directory from config: {output_dir}\")\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    output_dir.mkdir(\n",
    "        parents=True,    \n",
    "        exist_ok=True    \n",
    "    )\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    ''' \n",
    "    parents=True,    \n",
    "    # If parent directories don't exist, create them too\n",
    "                        # Example: if path is '/a/b/c' and 'a' and 'b' don't exist,\n",
    "                        # this will create all necessary parent directories\n",
    "    exist_ok=True\n",
    "    # If the directory already exists, don't raise an error\n",
    "                        # Without this, we'd get a FileExistsError if the directory exists\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # Resolve and check data paths\n",
    "    resolved_paths = {}\n",
    "    for key in ['train', 'validation', 'test']:\n",
    "        if key in data_cfg:\n",
    "            path_str = data_cfg[key] # for example, data_cfg['train'] = 'c:\\Users\\jiang\\Desktop\\Projects\\Lingo\\data\\processed\\text\\train.json'\n",
    "            if not os.path.isabs(path_str): # if the path is not absolute, in other words, the path is relative like data\\processed\\text\\train.json, we resolve it \n",
    "                path_obj = (project_root / path_str).resolve()\n",
    "                resolved_paths[key] = str(path_obj)\n",
    "            else:\n",
    "                path_obj = Path(path_str)\n",
    "                resolved_paths[key] = str(path_obj)\n",
    "            \n",
    "            print(f\"{key} path: {resolved_paths[key]}\")\n",
    "            print(f\"  - Exists: {path_obj.exists()}\")\n",
    "            \n",
    "        else:\n",
    "            if key in ['train', 'validation']: # test is optional here\n",
    "                print(f\"ERROR: Required data path for '{key}' missing!\")\n",
    "            else:\n",
    "                print(f\"NOTE: Optional data path for '{key}' not specified.\")\n",
    "    \n",
    "    # Update data_cfg with resolved paths\n",
    "    for key, path in resolved_paths.items():\n",
    "        data_cfg[key] = path\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing paths: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e1251",
   "metadata": {},
   "source": [
    "# 6. Load Tokenizer\n",
    "## Load the tokenizer for the specified model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5052d",
   "metadata": {},
   "source": [
    "Now we are talking about Qwen 2.5-1.5B as the specific model.\n",
    "\n",
    "\n",
    "**Why implement causal LM?** \n",
    "\n",
    "Model Architecture: Qwen 2.5 is fundamentally designed and pre-trained as a causal language model. This is how the model's architecture was built from the ground up - it was trained to predict the next token given previous tokens. We can't change this core architecture during fine-tuning.Therefore, we will use the causal LM. \n",
    "\n",
    "\n",
    "**why padding?**\n",
    "\n",
    "Most causal LM vocabularies (GPT‑style, Qwen‑style) don’t define a pad_token by default, because at pretraining time usually stream text without padding. Therefore, when fine‑tune or evaluate with batches of mixed lengths,we must pad shorter sequences up to your chosen max_length, or your tensors won’t line up.\n",
    "\n",
    "\n",
    "\n",
    "We set padding_side to left because this is more efficient for causal LM.\n",
    "\n",
    "**Why Left Padding for Causal LM?**\n",
    "\n",
    "For causal LMs, left padding is critical because:\n",
    "\n",
    "1. Attention Masking: In causal LMs, each token can only attend to previous tokens and itself (this is the \"causal\" part)\n",
    "\n",
    "2. With Right Padding: If padding is on the right, the model would try to use meaningful tokens to predict padding tokens, which is wasteful and potentially confusing:\n",
    "   ```\n",
    "   \"Hello world [PAD] [PAD]\"\n",
    "            ↑        ↑\n",
    "      Model tries to predict padding based on \"world\"\n",
    "   ```\n",
    "\n",
    "3. With Left Padding: The padding tokens come before the real content, so they don't interfere with prediction of meaningful tokens:\n",
    "   ```\n",
    "   \"[PAD] [PAD] Hello world\"\n",
    "              ↑       ↑\n",
    "       Model predicts \"world\" based on \"Hello\" (not padding)\n",
    "   ```\n",
    "\n",
    "4. Efficiency: Left padding allows the model to ignore the padding completely during generation, as attention to padding would only be relevant when predicting the first real token\n",
    "\n",
    "**Note**:Even though we're using a causal LM with left padding, the model still considers the entire input sentence for slang detection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5b5701b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Set padding_side to: left\n",
      "Tokenizer type: <class 'transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast'>\n",
      "Vocabulary size: 151665\n",
      "Model max length: 131072\n",
      "Tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load tokenizer\n",
    "    model_name = model_cfg[\"name\"]\n",
    "    print(f\"Loading tokenizer for: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    \n",
    "    # Padding need: Language models require inputs of uniform length in a batch. Since sentences have different lengths, we pad shorter ones to match the longest.\n",
    "    # Set padding token if not defined\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Set pad_token to eos_token: {tokenizer.eos_token}\")\n",
    "    \n",
    "    # Qwen2.5 is fundamentally \n",
    "    # Set padding side for causal LM\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    print(f\"Set padding_side to: {tokenizer.padding_side}\")\n",
    "    \n",
    "    # Print tokenizer info\n",
    "    print(f\"Tokenizer type: {type(tokenizer)}\")  # Shows exact tokenizer class\n",
    "    print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "    print(\"Tokenizer loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346b2ae",
   "metadata": {},
   "source": [
    "# ## 7. Load and Tokenize Datasets\n",
    "# Load the datasets and tokenize them for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2b9f7c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Raw datasets loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 9984\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 1248\n",
      "    })\n",
      "})\n",
      "Train dataset size: 9984\n",
      "Validation dataset size: 1248\n",
      "\n",
      "Sample training examples:\n",
      "Example 0:\n",
      "  Instruction: Identify any slang in this video subtitle: \"You gonna give me a ticket?\"\n",
      "  Output: slang detected: gonna\n",
      "slang context: You gonna give me a ticket?\n",
      "\n",
      "Example 1:\n",
      "  Instruction: Identify any slang in this video subtitle: \"No, the woman I lived with.\"\n",
      "  Output: no slang detected\n",
      "\n",
      "Example 2:\n",
      "  Instruction: Identify any slang in this video subtitle: \"And they still say it's impossible to get to the bottom of this.\"\n",
      "  Output: slang detected: bottom\n",
      "slang context: And they still say it's impossible to get to the bottom of this.\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 9984\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 1248\n",
      "})\n",
      "Tokenizing with max_length=512...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:34:15,036 - INFO - Tokenizing dataset with max_length=512...\n",
      "2025-04-21 20:34:15,037 - INFO - Using 8 processes for tokenization.\n",
      "2025-04-21 20:34:15,163 - INFO - Tokenization complete.\n",
      "2025-04-21 20:34:15,163 - INFO - Tokenizing dataset with max_length=512...\n",
      "2025-04-21 20:34:15,165 - INFO - Using 8 processes for tokenization.\n",
      "2025-04-21 20:34:15,285 - INFO - Tokenization complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset sizes: Train=9984, Validation=1248\n",
      "\n",
      "Sample tokenized example:\n",
      "<|im_start|>user\n",
      "Identify any slang in this video subtitle: \"You gonna give me a ticket?\"<|im_end|>\n",
      "<|im_start|>assistant\n",
      "slang detected: gonna\n",
      "slang context: You gonna give me a ticket?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load raw datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    data_files = {'train': data_cfg['train'], 'validation': data_cfg['validation']}\n",
    "    raw_datasets = load_dataset('json', data_files=data_files)\n",
    "    \n",
    "    print(f\"Raw datasets loaded: {raw_datasets}\")\n",
    "    print(f\"Train dataset size: {len(raw_datasets['train'])}\")\n",
    "    print(f\"Validation dataset size: {len(raw_datasets['validation'])}\")\n",
    "    \n",
    "    # Display a few examples\n",
    "    print(\"\\nSample training examples:\")\n",
    "    for i in range(min(3, len(raw_datasets['train']))):\n",
    "        print(f\"Example {i}:\")\n",
    "        print(f\"  Instruction: {raw_datasets['train'][i]['instruction']}\")\n",
    "        print(f\"  Output: {raw_datasets['train'][i]['output']}\")\n",
    "        print()\n",
    "    \n",
    "    # Reformat dataset if needed\n",
    "    def reformat_dataset(dataset):\n",
    "        return dataset.map(\n",
    "            lambda x: {\n",
    "                \"instruction\": x[\"instruction\"],\n",
    "                \"output\": x[\"output\"]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "    train_dataset = reformat_dataset(raw_datasets[\"train\"])\n",
    "    val_dataset = reformat_dataset(raw_datasets[\"validation\"])\n",
    "    print(f\"train_dataset: {train_dataset}\")\n",
    "    print(f\"val_dataset: {val_dataset}\" )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Tokenize datasets\n",
    "    max_length = train_cfg.get(\"max_length\", 512)\n",
    "    print(f\"Tokenizing with max_length={max_length}...\")\n",
    "    \n",
    "    train_ds = tokenize_dataset(train_dataset, tokenizer, max_length)\n",
    "    val_ds = tokenize_dataset(val_dataset, tokenizer, max_length)\n",
    "    \n",
    "    # Apply sample limits if specified\n",
    "    if max_train_samples:\n",
    "        train_ds = train_ds.select(range(min(max_train_samples, len(train_ds))))\n",
    "        print(f\"Limited training dataset to {len(train_ds)} samples\")\n",
    "    \n",
    "    if max_eval_samples:\n",
    "        val_ds = val_ds.select(range(min(max_eval_samples, len(val_ds))))\n",
    "        print(f\"Limited validation dataset to {len(val_ds)} samples\")\n",
    "    \n",
    "    print(f\"Final dataset sizes: Train={len(train_ds)}, Validation={len(val_ds)}\")\n",
    "    \n",
    "    # Show a tokenized example\n",
    "    print(\"\\nSample tokenized example:\")\n",
    "    sample_idx = 0\n",
    "    sample_ids = train_ds[sample_idx]['input_ids']\n",
    "    decoded = tokenizer.decode(sample_ids)\n",
    "    print(decoded)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading or tokenizing datasets: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "06b7acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Load raw datasets\n",
    "#     print(\"Loading datasets...\")\n",
    "#     data_files = {'train': data_cfg['train'], 'validation': data_cfg['validation']}\n",
    "#     raw_datasets = load_dataset('json', data_files=data_files)\n",
    "    \n",
    "#     print(f\"Raw datasets loaded: {raw_datasets}\")\n",
    "#     print(f\"Train dataset size: {len(raw_datasets['train'])}\")\n",
    "#     print(f\"Validation dataset size: {len(raw_datasets['validation'])}\")\n",
    "    \n",
    "#     # ADDED: Raw dataset structure debugging\n",
    "#     print(\"\\nRaw dataset structure check:\")\n",
    "#     print(f\"First example keys: {list(raw_datasets['train'][0].keys())}\")\n",
    "#     print(f\"First example content: {raw_datasets['train'][0]}\")\n",
    "    \n",
    "#     # Display a few examples\n",
    "#     print(\"\\nSample training examples:\")\n",
    "#     for i in range(min(1, len(raw_datasets['train']))):\n",
    "#         print(f\"Example {i}:\")\n",
    "#         print(f\"  Instruction: {raw_datasets['train'][i]['instruction']}\")\n",
    "#         print(f\"  Output: {raw_datasets['train'][i]['output']}\")\n",
    "#         print()\n",
    "    \n",
    "#     # Reformat dataset if needed\n",
    "#     def reformat_dataset(dataset):\n",
    "#         # ADDED: Debugging before reformatting\n",
    "#         print(f\"Reformatting dataset with {len(dataset)} examples\")\n",
    "#         if len(dataset) > 0:\n",
    "#             print(f\"First example keys before reformatting: {list(dataset[0].keys())}\")\n",
    "            \n",
    "#         # Original code\n",
    "#         reformatted = dataset.map(\n",
    "#             lambda x: {\n",
    "#                 \"instruction\": x[\"instruction\"],\n",
    "#                 \"output\": x[\"output\"]\n",
    "#             }\n",
    "#         )\n",
    "        \n",
    "#         # ADDED: Debugging after reformatting\n",
    "#         print(f\"Reformatted dataset size: {len(reformatted)}\")\n",
    "#         if len(reformatted) > 0:\n",
    "#             print(f\"First example keys after reformatting: {list(reformatted[0].keys())}\")\n",
    "            \n",
    "#         return reformatted\n",
    "#     print(\"-------------------------------- \")\n",
    "#     print(\"--------------\")\n",
    "#     print(\"--------------\")\n",
    "#     print(\"--------------\")\n",
    "\n",
    "#     train_dataset = reformat_dataset(raw_datasets[\"train\"])\n",
    "#     val_dataset = reformat_dataset(raw_datasets[\"validation\"])\n",
    "    \n",
    "#     # ADDED: More debugging\n",
    "    \n",
    "#     print(f\"train_dataset type: {type(train_dataset)}\")\n",
    "#     print(f\"val_dataset type: {type(val_dataset)}\" )\n",
    "\n",
    "#     print(\"-------------------------------- \")\n",
    "#     print(\"--------------\")\n",
    "#     print(\"--------------\")\n",
    "#     print(\"--------------\")\n",
    "\n",
    "#     print(train_dataset)\n",
    "\n",
    "\n",
    "# # After this debug chunk, I know that the problem is in tokenize_dataset function defined above, so now I want to go back and rewrite tokenize_dataset() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45b884",
   "metadata": {},
   "source": [
    "# Understanding the Prompt Format for Qwen2.5-Instruct\n",
    "\n",
    "Yes, the prompt format you're seeing is specific to the Qwen2.5-Instruct model. This is a chat-based instruction-tuning format that the model was trained on.\n",
    "\n",
    "## The Prompt Structure\n",
    "\n",
    "The format `<|im_start|>user ... <|im_end|> <|im_start|>assistant ... <|im_end|>` is a special chat template that Qwen2.5-Instruct understands:\n",
    "\n",
    "1. `<|im_start|>user` - Indicates the beginning of user input\n",
    "2. `<|im_end|>` - Indicates the end of user input\n",
    "3. `<|im_start|>assistant` - Indicates the beginning of assistant (model) output\n",
    "4. `<|im_end|>` - Indicates the end of assistant output\n",
    "\n",
    "These special tokens help the model distinguish between:\n",
    "\n",
    "* What part of the text is the instruction/question (user part)\n",
    "* What part of the text is the expected response (assistant part)\n",
    "\n",
    "## Why This Format Is Used\n",
    "\n",
    "This format is crucial because:\n",
    "\n",
    "1. Model Conditioning: It conditions the model to understand the role separation between user and assistant\n",
    "2. Fine-tuning Alignment: When fine-tuning, this format helps the model learn to generate responses in the assistant's voice after seeing user instructions\n",
    "3. Consistent with Pre-training: The model was pre-trained to recognize these special tokens and respond accordingly\n",
    "\n",
    "## In Your Specific Case\n",
    "\n",
    "For your slang detection task:\n",
    "\n",
    "* User part: Contains the instruction and the text to analyze (`\"Identify any slang in this video subtitle: \"You gonna give me a ticket?\"\"`)\n",
    "* Assistant part: Contains the expected output (`\"slang detected: gonna\\nslang context: You gonna give me a ticket?\"`)\n",
    "\n",
    "When fine-tuning, this teaches the model to:\n",
    "\n",
    "1. Recognize the instruction pattern\n",
    "2. Identify slang in the provided text\n",
    "3. Format the response in the expected way\n",
    "\n",
    "This chat-based format is common across many instruction-tuned models, though the exact tokens may differ (some use `<s>`, `</s>`, `[INST]`, etc.). Qwen2.5 specifically uses the `<|im_start|>` and `<|im_end|>` tokens to mark the boundaries of different speakers in the conversation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8215d",
   "metadata": {},
   "source": [
    "### Understanding Qwen2.5 Formatting Requirements\n",
    "\n",
    "**Why We're Manually Adding Special Tokens**\n",
    "\n",
    "The special tokens (`<|im_start|>`, `<|im_end|>`) are added manually because this is the specific chat format that Qwen2.5-Instruct expects. This format is documented in the Qwen2.5 model card and documentation.\n",
    "\n",
    "According to the Qwen2.5 documentation, the model expects inputs in this chat format:\n",
    "\n",
    "```python\n",
    "<|im_start|>user\n",
    "{user message}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant message}<|im_end|>\n",
    "```\n",
    "\n",
    "This is not something we're inventing - it's the official format recommended by the model creators.\n",
    "\n",
    "## About the Reformatting Step\n",
    "\n",
    "Looking at your dataset sample, I can see why you're confused. Your dataset is already well-structured with:\n",
    "\n",
    "* `instruction`: The task instruction\n",
    "* `input`: An empty field (not used in this case)\n",
    "* `output`: The expected response\n",
    "\n",
    "The reformatting step in your code:\n",
    "\n",
    "```python\n",
    "def reformat_dataset(dataset):\n",
    "    reformatted = dataset.map(\n",
    "        lambda x: {\n",
    "            \"instruction\": x[\"instruction\"],\n",
    "            \"output\": x[\"output\"]\n",
    "        }\n",
    "    )\n",
    "    return reformatted\n",
    "```\n",
    "\n",
    "This step is actually redundant for your dataset because:\n",
    "1. Your data already has the fields we need\n",
    "2. The reformatting is just selecting the same fields that already exist\n",
    "\n",
    "You could safely remove this reformatting step since your dataset is already in the right structure. It's likely a leftover from a more general pipeline that might handle datasets with different field names.\n",
    "\n",
    "## Qwen2.5's Dataset Format Requirements\n",
    "\n",
    "Qwen2.5-Instruct was fine-tuned on a dataset that follows this general structure:\n",
    "\n",
    "1. Raw data format: Similar to your JSON with instruction/output pairs\n",
    "2. Tokenization format: The raw data converted to the chat format with special tokens\n",
    "\n",
    "The key transformation happens when we convert from your JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"Identify any slang...\",\n",
    "    \"output\": \"slang detected: gonna...\"\n",
    "}\n",
    "```\n",
    "\n",
    "To the chat format:\n",
    "\n",
    "```\n",
    "<|im_start|>user\n",
    "Identify any slang...\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "slang detected: gonna...\n",
    "<|im_end|>\n",
    "```\n",
    "\n",
    "This transformation is what the `create_prompt` function does. It's not redoing your dataset preparation - it's just formatting it in the way Qwen2.5 expects for training.\n",
    "\n",
    "## Clarification on Your Dataset\n",
    "\n",
    "Your dataset preparation work was valuable and correct! You created a well-structured dataset with instruction/output pairs. The reformatting step in the code is just ensuring consistency, and the tokenization step is converting it to Qwen's expected format with the special tokens.\n",
    "\n",
    "If you want to simplify your pipeline, you could remove the redundant reformatting step since your data is already properly structured.\n",
    "```\n",
    "\n",
    "This markdown should render correctly in Google Colab, with proper code block formatting and structure. The code examples are properly fenced and syntax-highlighted where appropriate. Let me know if you need any adjustments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4efda67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:34:15,540 - INFO - Tokenizing dataset with max_length=512...\n",
      "2025-04-21 20:34:15,541 - INFO - Using 8 processes for tokenization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw datasets loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 9984\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 1248\n",
      "    })\n",
      "})\n",
      "Train dataset size: 9984\n",
      "Validation dataset size: 1248\n",
      "\n",
      "Raw dataset structure check:\n",
      "First example keys: ['instruction', 'input', 'output']\n",
      "First example content: {'instruction': 'Identify any slang in this video subtitle: \"You gonna give me a ticket?\"', 'input': '', 'output': 'slang detected: gonna\\nslang context: You gonna give me a ticket?'}\n",
      "\n",
      "Sample training examples:\n",
      "Example 0:\n",
      "  Instruction: Identify any slang in this video subtitle: \"You gonna give me a ticket?\"\n",
      "  Output: slang detected: gonna\n",
      "slang context: You gonna give me a ticket?\n",
      "\n",
      "Reformatting dataset with 9984 examples\n",
      "First example keys before reformatting: ['instruction', 'input', 'output']\n",
      "Reformatted dataset size: 9984\n",
      "First example keys after reformatting: ['instruction', 'input', 'output']\n",
      "Reformatting dataset with 1248 examples\n",
      "First example keys before reformatting: ['instruction', 'input', 'output']\n",
      "Reformatted dataset size: 1248\n",
      "First example keys after reformatting: ['instruction', 'input', 'output']\n",
      "train_dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "val_dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Tokenizing with max_length=512...\n",
      "Starting tokenization of dataset with 9984 examples\n",
      "Dataset columns: ['instruction', 'input', 'output']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:34:15,696 - INFO - Tokenization complete.\n",
      "2025-04-21 20:34:15,698 - INFO - Tokenizing dataset with max_length=512...\n",
      "2025-04-21 20:34:15,699 - INFO - Using 8 processes for tokenization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Result size: 9984\n",
      "First tokenized example keys: ['input_ids', 'attention_mask', 'labels']\n",
      "Starting tokenization of dataset with 1248 examples\n",
      "Dataset columns: ['instruction', 'input', 'output']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 20:34:15,856 - INFO - Tokenization complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Result size: 1248\n",
      "First tokenized example keys: ['input_ids', 'attention_mask', 'labels']\n",
      "Final dataset sizes: Train=9984, Validation=1248\n",
      "\n",
      "\n",
      "\n",
      "Sample tokenized example:\n",
      "Available keys in first example: ['input_ids', 'attention_mask', 'labels']\n",
      "\n",
      "0. ORIGINAL TEXT (BEFORE TOKENIZATION):\n",
      "----------------------------------------\n",
      "Instruction: Identify any slang in this video subtitle: \"You gonna give me a ticket?\"\n",
      "Output: slang detected: gonna\n",
      "slang context: You gonna give me a ticket?\n",
      "\n",
      "Combined into prompt:\n",
      "<|im_start|>user\n",
      "Identify any slang in this video subtitle: \"You gonna give me a ticket?\"<|im_end|>\n",
      "<|im_start|>assistant\n",
      "slang detected: gonna\n",
      "slang context: You gonna give me a ticket?<|im_end|>\n",
      "----------------------------------------\n",
      "\n",
      "1. DECODED TEXT (AFTER TOKENIZATION):\n",
      "----------------------------------------\n",
      "<|im_start|>user\n",
      "Identify any slang in this video subtitle: \"You gonna give me a ticket?\"<|im_end|>\n",
      "<|im_start|>assistant\n",
      "slang detected: gonna\n",
      "slang context: You gonna give me a ticket?<|im_end|>\n",
      "----------------------------------------\n",
      "\n",
      "2. TOKEN IDs (WHAT THE MODEL ACTUALLY SEES):\n",
      "----------------------------------------\n",
      "First 20 tokens: [151644, 872, 198, 28301, 1437, 894, 79912, 304, 419, 2766, 31735, 25, 330, 2610, 16519, 2968, 752, 264, 11727, 7521]\n",
      "Last 20 tokens: [77091, 198, 3226, 524, 16507, 25, 16519, 198, 3226, 524, 2266, 25, 1446, 16519, 2968, 752, 264, 11727, 30, 151645]\n",
      "Total tokens: 43\n",
      "----------------------------------------\n",
      "\n",
      "3. TOKEN-BY-TOKEN BREAKDOWN (FIRST 20 TOKENS):\n",
      "----------------------------------------\n",
      "Index    Token ID   Token Text                    \n",
      "------------------------------------------------------------\n",
      "0        151644     '<|im_start|>'                \n",
      "1        872        'user'                        \n",
      "2        198        '\\n'                          \n",
      "3        28301      'Ident'                       \n",
      "4        1437       'ify'                         \n",
      "5        894        ' any'                        \n",
      "6        79912      ' slang'                      \n",
      "7        304        ' in'                         \n",
      "8        419        ' this'                       \n",
      "9        2766       ' video'                      \n",
      "10       31735      ' subtitle'                   \n",
      "11       25         ':'                           \n",
      "12       330        ' \"'                          \n",
      "13       2610       'You'                         \n",
      "14       16519      ' gonna'                      \n",
      "15       2968       ' give'                       \n",
      "16       752        ' me'                         \n",
      "17       264        ' a'                          \n",
      "18       11727      ' ticket'                     \n",
      "19       7521       '?\"'                          \n",
      "----------------------------------------\n",
      "\n",
      "4. COMPARISON (ORIGINAL vs DECODED):\n",
      "----------------------------------------\n",
      "Are they identical?  True\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load raw datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    data_files = {'train': data_cfg['train'], 'validation': data_cfg['validation']}\n",
    "    raw_datasets = load_dataset('json', data_files=data_files)\n",
    "    \n",
    "    print(f\"Raw datasets loaded: {raw_datasets}\")\n",
    "    print(f\"Train dataset size: {len(raw_datasets['train'])}\")\n",
    "    print(f\"Validation dataset size: {len(raw_datasets['validation'])}\")\n",
    "    \n",
    "    # ADDED: Raw dataset structure debugging\n",
    "    print(\"\\nRaw dataset structure check:\")\n",
    "    print(f\"First example keys: {list(raw_datasets['train'][0].keys())}\")\n",
    "    print(f\"First example content: {raw_datasets['train'][0]}\")\n",
    "    \n",
    "    # Display a few examples\n",
    "    print(\"\\nSample training examples:\")\n",
    "    for i in range(min(1, len(raw_datasets['train']))):\n",
    "        print(f\"Example {i}:\")\n",
    "        print(f\"  Instruction: {raw_datasets['train'][i]['instruction']}\")\n",
    "        print(f\"  Output: {raw_datasets['train'][i]['output']}\")\n",
    "        print()\n",
    "    \n",
    "    # Reformat dataset if needed\n",
    "    def reformat_dataset(dataset):\n",
    "        # ADDED: Debugging before reformatting\n",
    "        print(f\"Reformatting dataset with {len(dataset)} examples\")\n",
    "        if len(dataset) > 0:\n",
    "            print(f\"First example keys before reformatting: {list(dataset[0].keys())}\")\n",
    "            \n",
    "        # Original code\n",
    "        reformatted = dataset.map(\n",
    "            lambda x: {\n",
    "                \"instruction\": x[\"instruction\"],\n",
    "                \"output\": x[\"output\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # ADDED: Debugging after reformatting\n",
    "        print(f\"Reformatted dataset size: {len(reformatted)}\")\n",
    "        if len(reformatted) > 0:\n",
    "            print(f\"First example keys after reformatting: {list(reformatted[0].keys())}\")\n",
    "            \n",
    "        return reformatted\n",
    "\n",
    "    train_dataset = reformat_dataset(raw_datasets[\"train\"])\n",
    "    val_dataset = reformat_dataset(raw_datasets[\"validation\"])\n",
    "    \n",
    "    # ADDED: More debugging\n",
    "    print(f\"train_dataset type: {type(train_dataset)}\")\n",
    "    print(f\"val_dataset type: {type(val_dataset)}\" )\n",
    "\n",
    "    # Tokenize datasets\n",
    "    max_length = train_cfg.get(\"max_length\", 512)\n",
    "    print(f\"Tokenizing with max_length={max_length}...\")\n",
    "    \n",
    "    # ADDED: Defensive tokenization wrapper\n",
    "    def tokenize_with_logging(dataset, tokenizer, max_length):\n",
    "        \"\"\"Wrapper around tokenize_dataset with added logging\"\"\"\n",
    "        print(f\"Starting tokenization of dataset with {len(dataset)} examples\")\n",
    "        print(f\"Dataset columns: {dataset.column_names}\")\n",
    "        \n",
    "        # Call the original tokenize_dataset function\n",
    "        result = tokenize_dataset(dataset, tokenizer, max_length)\n",
    "        \n",
    "        # Check the result\n",
    "        if result is None:\n",
    "            print(\"ERROR: tokenize_dataset returned None\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"Tokenization complete. Result size: {len(result)}\")\n",
    "        if len(result) > 0:\n",
    "            print(f\"First tokenized example keys: {list(result[0].keys())}\")\n",
    "        else:\n",
    "            print(\"WARNING: Empty tokenized dataset returned\")\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    # Replace direct calls with wrapped version\n",
    "    train_ds = tokenize_with_logging(train_dataset, tokenizer, max_length)\n",
    "    val_ds = tokenize_with_logging(val_dataset, tokenizer, max_length)\n",
    "    \n",
    "    # Apply sample limits if specified\n",
    "    if max_train_samples and train_ds is not None:\n",
    "        train_ds = train_ds.select(range(min(max_train_samples, len(train_ds))))\n",
    "        print(f\"Limited training dataset to {len(train_ds)} samples\")\n",
    "    \n",
    "    if max_eval_samples and val_ds is not None:\n",
    "        val_ds = val_ds.select(range(min(max_eval_samples, len(val_ds))))\n",
    "        print(f\"Limited validation dataset to {len(val_ds)} samples\")\n",
    "    \n",
    "    # MODIFIED: Added defensive checks\n",
    "    train_size = len(train_ds) if train_ds is not None else 0\n",
    "    val_size = len(val_ds) if val_ds is not None else 0\n",
    "    print(f\"Final dataset sizes: Train={train_size}, Validation={val_size}\")\n",
    "    \n",
    "    # MODIFIED: Show a tokenized example with defensive checks\n",
    "    print(\"\\n\\n\\nSample tokenized example:\")\n",
    "\n",
    "    ######\n",
    "    #####\n",
    "    ######\n",
    "    \n",
    "    \n",
    "    # #simplified version without much print() statements\n",
    "    # if train_ds is not None and len(train_ds) > 0:\n",
    "    #     sample_idx = 0\n",
    "    #     print(f\"Available keys in first example: {list(train_ds[0].keys())}\")\n",
    "    #     if 'input_ids' in train_ds[sample_idx]:\n",
    "    #         sample_ids = train_ds[sample_idx]['input_ids']\n",
    "    #         decoded = tokenizer.decode(sample_ids)\n",
    "    #         print(decoded)\n",
    "    #     else:\n",
    "    #         print(\"ERROR: 'input_ids' not found in the first example\")\n",
    "    # else:\n",
    "    #     print(\"ERROR: No examples in the tokenized dataset\")\n",
    "\n",
    "\n",
    "    ######################\n",
    "    # ##################\n",
    "    #     \n",
    "    if train_ds is not None and len(train_ds) > 0 and train_dataset is not None:\n",
    "        sample_idx = 0\n",
    "        print(f\"Available keys in first example: {list(train_ds[0].keys())}\")\n",
    "        \n",
    "        # Get the original text from the pre-tokenized dataset\n",
    "        original_instruction = train_dataset[sample_idx]['instruction']\n",
    "        original_output = train_dataset[sample_idx]['output']\n",
    "        \n",
    "        # 0. Show the original text before tokenization\n",
    "        print(\"\\n0. ORIGINAL TEXT (BEFORE TOKENIZATION):\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"Instruction: {original_instruction}\")\n",
    "        print(f\"Output: {original_output}\")\n",
    "        \n",
    "        # Create the prompt manually to show exactly what gets tokenized\n",
    "        original_prompt = f\"<|im_start|>user\\n{original_instruction}<|im_end|>\\n<|im_start|>assistant\\n{original_output}<|im_end|>\"\n",
    "        print(\"\\nCombined into prompt:\")\n",
    "        print(original_prompt)\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        if 'input_ids' in train_ds[sample_idx]:\n",
    "            # Get the tokenized IDs\n",
    "            sample_ids = train_ds[sample_idx]['input_ids']\n",
    "            \n",
    "            # 1. Show decoded text from tokens\n",
    "            decoded = tokenizer.decode(sample_ids)\n",
    "            print(\"\\n1. DECODED TEXT (AFTER TOKENIZATION):\")\n",
    "            print(\"-\"*40)\n",
    "            print(decoded)\n",
    "            print(\"-\"*40)\n",
    "            \n",
    "            # 2. Show token IDs\n",
    "            print(\"\\n2. TOKEN IDs (WHAT THE MODEL ACTUALLY SEES):\")\n",
    "            print(\"-\"*40)\n",
    "            # Show first 20 tokens\n",
    "            print(f\"First 20 tokens: {sample_ids[:20]}\")\n",
    "            # Show last 20 tokens if sequence is long enough\n",
    "            if len(sample_ids) > 40:\n",
    "                print(f\"Last 20 tokens: {sample_ids[-20:]}\")\n",
    "            print(f\"Total tokens: {len(sample_ids)}\")\n",
    "            print(\"-\"*40)\n",
    "            \n",
    "            # 3. Show token-by-token breakdown\n",
    "            print(\"\\n3. TOKEN-BY-TOKEN BREAKDOWN (FIRST 20 TOKENS):\")\n",
    "            print(\"-\"*40)\n",
    "            print(f\"{'Index':<8} {'Token ID':<10} {'Token Text':<30}\")\n",
    "            print(\"-\"*60)\n",
    "            for i, token_id in enumerate(sample_ids[:20]):\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                print(f\"{i:<8} {token_id:<10} {repr(token_text):<30}\")\n",
    "            print(\"-\"*40)\n",
    "            \n",
    "            # 4. Compare original and decoded\n",
    "            print(\"\\n4. COMPARISON (ORIGINAL vs DECODED):\")\n",
    "            print(\"-\"*40)\n",
    "            print(\"Are they identical? \", original_prompt == decoded)\n",
    "            if original_prompt != decoded:\n",
    "                print(\"\\nDifferences might be due to tokenization and detokenization process.\")\n",
    "                print(\"This is normal as tokenizers may normalize text, handle whitespace differently, etc.\")\n",
    "            print(\"-\"*40)\n",
    "        else:\n",
    "            print(\"ERROR: 'input_ids' not found in the first example\")\n",
    "    else:\n",
    "        print(\"ERROR: No examples in the tokenized dataset\")\n",
    "\n",
    "\n",
    "\n",
    "    #######\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or tokenizing datasets: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b9021",
   "metadata": {},
   "source": [
    "# ## 8. Load Base Model with Quantization\n",
    "# Load the model with 4-bit quantization to fit in limited GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c92089f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up 4-bit quantization...\n",
      "bitsandbytes library found.\n",
      "4-bit quantization config: {'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>, '_load_in_8bit': False, '_load_in_4bit': True, 'llm_int8_threshold': 6.0, 'llm_int8_skip_modules': None, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'load_in_4bit': True, 'load_in_8bit': False}\n",
      "WARNING: No GPU detected! Training will be very slow.\n",
      "Loading base model: Qwen/Qwen2.5-1.5B-Instruct...\n",
      "This may take several minutes...\n"
     ]
    }
   ],
   "source": [
    "# ## 8. Load Base Model with Quantization\n",
    "# Load the model with 4-bit quantization to fit in limited GPU memory\n",
    "\n",
    "# %%\n",
    "try:\n",
    "    # Setup model loading arguments\n",
    "    model_load_kwargs = {\"trust_remote_code\": True}\n",
    "    \n",
    "    if use_4bit:\n",
    "        print(\"Setting up 4-bit quantization...\")\n",
    "        try:\n",
    "            import bitsandbytes\n",
    "            print(\"bitsandbytes library found.\")\n",
    "        except ImportError:\n",
    "            print(\"ERROR: bitsandbytes library not found. Please install with: pip install bitsandbytes\")\n",
    "            raise\n",
    "        \n",
    "        # Create quantization config\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if bf16 else torch.float16\n",
    "        )\n",
    "        model_load_kwargs[\"quantization_config\"] = quantization_config\n",
    "        model_load_kwargs[\"device_map\"] = \"auto\"\n",
    "        print(f\"4-bit quantization config: {quantization_config.to_dict()}\")\n",
    "    else:\n",
    "        # Standard loading with selected precision\n",
    "        model_load_kwargs[\"torch_dtype\"] = torch.bfloat16 if bf16 else torch.float16\n",
    "        model_load_kwargs[\"device_map\"] = \"auto\"\n",
    "        print(f\"Loading with torch_dtype: {model_load_kwargs['torch_dtype']}\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"WARNING: No GPU detected! Training will be very slow.\")\n",
    "    \n",
    "    # Load base model\n",
    "    print(f\"Loading base model: {model_cfg['name']}...\")\n",
    "    print(\"This may take several minutes...\")\n",
    "    \n",
    "    # This cell might take a long time to execute\n",
    "    \n",
    "    # Comment these lines if we don't want to actually load the model\n",
    "    # base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_cfg[\"name\"],\n",
    "    #     **model_load_kwargs\n",
    "    # )\n",
    "    # print(\"Base model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a332dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "231fa1d1",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f737652",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(model_cfg['output_dir'])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    per_device_train_batch_size=train_cfg['batch_size'],\n",
    "    per_device_eval_batch_size=train_cfg.get('eval_batch_size', train_cfg['batch_size']*2),\n",
    "    gradient_accumulation_steps=train_cfg['gradient_accumulation_steps'],\n",
    "    num_train_epochs=train_cfg['epochs'],\n",
    "    learning_rate=train_cfg['lr'],\n",
    "    warmup_ratio=train_cfg['warmup_ratio'],\n",
    "    fp16=not BF16,\n",
    "    bf16=BF16,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=train_cfg.get('logging_steps', 50),\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  tokenizer=tok,\n",
    "                  args=args,\n",
    "                  train_dataset=train_ds,\n",
    "                  eval_dataset=val_ds,\n",
    "                  data_collator=collator)\n",
    "\n",
    "# Uncomment to run\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e0109",
   "metadata": {},
   "source": [
    "## 8. Save Model & Optional Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa65a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()\n",
    "# tok.save_pretrained(str(output_dir))\n",
    "\n",
    "# if 'test' in data_cfg and Path(data_cfg['test']).exists():\n",
    "#     test_raw = load_dataset('json', data_files={'test': data_cfg['test']})\n",
    "#     test_ds  = tokenize_dataset(test_raw['test'], tok, train_cfg.get('max_length', 512))\n",
    "#     trainer.evaluate(eval_dataset=test_ds, metric_key_prefix='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
